{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"15.앙상블.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyM33Kta8SCawyRHa+xY3hg1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-09-22T06:10:18.453277Z","start_time":"2021-09-22T06:10:12.756312Z"},"id":"dcSXnvzQaNcd"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.ensemble import BaggingClassifier,RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier \n","\n","\n","from sklearn.datasets import load_breast_cancer, load_wine\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","\n","from warnings import filterwarnings\n","filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"zGi78WSPaPmY"},"source":["앙상블 학습의 유형: 보팅(Voting), 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking) 등"]},{"cell_type":"markdown","metadata":{"id":"5n9j9JQknyiF"},"source":["# 1.Voting\n","\n","- 여러 종류의 알고리즘을 사용한 각각의 결과에 대해 투표를 통해 최종 결과를 예측하는 방식\n","- 하드 보팅: 다수결의 원칙\n","- 소프트 보팅: 각 알고리즘이 레이블값 결정 확률을 예측한 결과를 평균내어 확률이 가장 높은 레이블값을 최종 값으로 예측\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCmw3BjcaTpI"},"outputs":[],"source":["cancer = load_breast_cancer()\n","cancer_df = pd.DataFrame(cancer['data'], columns = cancer['feature_names'])\n","cancer_df['target'] = cancer['target']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NL2F3hfnyiG"},"outputs":[],"source":["x_data = cancer['data']\n","y_data = cancer['target']\n","\n","x_train, x_test, y_train, y_test = train_test_split(x_data,y_data, test_size = 0.2,\n","                                                   random_state = 1, stratify = y_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a89yNER3nyiI"},"outputs":[],"source":["model_logi = make_pipeline(StandardScaler(),LogisticRegression())\n","model_knn = make_pipeline(StandardScaler(),KNeighborsClassifier())\n","model_tree = make_pipeline(StandardScaler(),DecisionTreeClassifier())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c36bad39","outputId":"b5260ab1-1e8c-4748-fdf0-999936aeb6cf"},"outputs":[{"data":{"text/plain":["VotingClassifier(estimators=[('logi', LogisticRegression()),\n","                             ('knn', KNeighborsClassifier()),\n","                             ('tree', DecisionTreeClassifier())])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# voting = 'hard' or 'soft'\n","# hard 는 다수결 / soft는 확률\n","model_vote = VotingClassifier(estimators = [('logi',model_logi),('knn',model_knn),('tree',model_tree)])\n","\n","model_vote.fit( x_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8318851a","outputId":"dbc6e1a5-a696-4816-9bfe-9c77a705fac8"},"outputs":[{"data":{"text/plain":["0.956140350877193"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model_logi.fit( x_train, y_train)\n","model_logi.predict( x_test)\n","model_logi.score( x_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10de2de2","outputId":"24cb91a7-0875-4a0b-b890-1493f2c9d818"},"outputs":[{"data":{"text/plain":["0.956140350877193"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model_knn.fit(x_train, y_train)\n","model_knn.predict( x_test)\n","model_knn.score( x_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8317ef7","outputId":"68325d37-8180-43dd-b80d-c48a55b3e33c"},"outputs":[{"data":{"text/plain":["0.9649122807017544"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model_vote.predict( x_test )\n","model_vote.score( x_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bb4e4a36","outputId":"5f1e4ed0-cef9-475c-d7bf-a0d2f65bbea6"},"outputs":[{"data":{"text/plain":["0.9649122807017544"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["p =model_vote.predict( x_test )\n","( y_test == p ).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7eaaa8e1","outputId":"20252737-7e37-4548-f3e5-2610cbe0b77d"},"outputs":[{"name":"stdout","output_type":"stream","text":["LogisticRegression 0.956140350877193\n","KNeighborsClassifier 0.956140350877193\n","DecisionTreeClassifier 0.9473684210526315\n"]}],"source":["for c in [model_logi, model_knn, model_tree]:\n","    c.fit( x_train, y_train)\n","    print( c.__class__.__name__, c.score( x_test, y_test))"]},{"cell_type":"markdown","source":["## 1-1.연습문제\n","\n","와인데이터셋에 대해 3개의 분류 클래스를 이용하고 soft voting 으로 정확도를 구하시오."],"metadata":{"id":"sYvnyqH2IsXU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5b737c0c"},"outputs":[],"source":["from sklearn.datasets import load_wine\n","wine = load_wine()\n","x_data = wine['data']\n","y_data = wine['target']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"670bad4f"},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2,\n","                                                    stratify=y_data, random_state=1 )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6f834d30"},"outputs":[],"source":["from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13c4de82"},"outputs":[],"source":["# 로지스틱, knn, decision tree 모델 객체 생성\n","model_logi = make_pipeline( StandardScaler(),  LogisticRegression() )\n","model_knn =  make_pipeline( StandardScaler(), KNeighborsClassifier() )\n","model_tree = make_pipeline( StandardScaler(),  DecisionTreeClassifier() )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3008ed27","outputId":"cea3775d-e465-40c6-c426-350ff7220772"},"outputs":[{"data":{"text/plain":["VotingClassifier(estimators=[('logi',\n","                              Pipeline(steps=[('standardscaler',\n","                                               StandardScaler()),\n","                                              ('logisticregression',\n","                                               LogisticRegression())])),\n","                             ('knn',\n","                              Pipeline(steps=[('standardscaler',\n","                                               StandardScaler()),\n","                                              ('kneighborsclassifier',\n","                                               KNeighborsClassifier())])),\n","                             ('tree',\n","                              Pipeline(steps=[('standardscaler',\n","                                               StandardScaler()),\n","                                              ('decisiontreeclassifier',\n","                                               DecisionTreeClassifier())]))],\n","                 voting='soft')"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["model_vote=VotingClassifier(estimators=[('logi',model_logi),('knn', model_knn),('tree',model_tree)],\n","                            voting='soft')\n","model_vote.fit(x_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b10767a","outputId":"59c39dc8-d918-4584-b5e2-b5e58b96200d"},"outputs":[{"data":{"text/plain":["1.0"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["model_vote.score( x_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"L_2agemenyiM"},"source":["# 2.Bagging\n","\n","- bagging은 bootstrap aggregating의 줄임말\n","- bootstrap:모집단의 성질에 대해 표본을 통해 추정할 수 있는 것처럼, 표본의 성질에 대해서도 재표집(resampling)을 통해 추정할 수 있다는 것이다. 즉 주어진 표본(샘플)에 대해서, 그 샘플에서 또 다시 샘플(재표본)을 여러번(1,000~10,000번, 혹은 그 이상)추출하여 표본의 평균이나 분산 등이 어떤 분포를 가지는가를 알아낼 수 있다.(위키피디아)\n","- 같은 알고리즘에 대해 데이터 샘플을 다르게 두고 학습을 수행해 보팅을 수행하는 방식\n","- 이 때의 데이터 샘플은 중첩이 허용된다. 즉 10000개의 데이터에 대해 10개의 알고리즘이 배깅을 사용할 때,각 1000개의 데이터 내에는 중복된 데이터가 존재할 수 있다. \n","- 배깅의 대표적인 방식이 Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87BwuTbpnyiM"},"outputs":[],"source":["wine = load_wine()\n","\n","x_data = wine['data']\n","y_data = wine['target']\n","\n","x_train, x_test, y_train, y_test = train_test_split(x_data,y_data, test_size = 0.2,\n","                                                   random_state = 1, stratify = y_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsuVJpJNnyiN"},"outputs":[],"source":["model_knn = make_pipeline(StandardScaler(),KNeighborsClassifier())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKiFvMfBnyiN","outputId":"606b298b-83f3-4110-b994-6c2c868ebd37"},"outputs":[{"data":{"text/plain":["BaggingClassifier(base_estimator=Pipeline(steps=[('standardscaler',\n","                                                  StandardScaler()),\n","                                                 ('kneighborsclassifier',\n","                                                  KNeighborsClassifier())]),\n","                  max_samples=0.5, random_state=1)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model_bagg = BaggingClassifier(model_knn, n_estimators=10,  # knn모델이 10개가 되는거임\n","                              max_samples = 0.5, random_state=1)\n","\n","# knn모델을 10개 만드는데 샘플을 50%로 나눠주는 거임\n","# 샘플을 랜덤하고 다양하게 나눔\n","\n","model_bagg.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fv3DTLApnyiN","outputId":"ced3b81f-e5e0-48a6-dcc9-81eddf1412e5"},"outputs":[{"data":{"text/plain":["array([1, 1, 2, 0, 2, 0, 2, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 1, 0,\n","       2, 0, 1, 0, 0, 1, 0, 0, 2, 1, 2, 2, 2, 0])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model_bagg.predict(x_test) # 다수결의 원칙을 사용함"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppw3FGjInyiN","outputId":"ae458580-c476-4cec-e702-d4a756f99671"},"outputs":[{"data":{"text/plain":["0.9444444444444444"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model_bagg.score(x_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5qKe_zwnyiO","outputId":"54eaadf8-f61a-4e21-ed82-6054ca3e31f5"},"outputs":[{"data":{"text/plain":["array([1, 1, 2, 0, 2, 0, 2, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0,\n","       2, 0, 1, 0, 0, 1, 0, 0, 2, 1, 2, 2, 2, 0])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["model_bagg = BaggingClassifier(model_knn, n_estimators=10,\n","                              max_samples = 0.5, random_state=2)\n","model_bagg.fit(x_train,y_train)\n","\n","model_bagg.predict(x_test) "]},{"cell_type":"markdown","metadata":{"id":"GlBW-8yenyiO"},"source":["# 3.RandomForest(Decision Tree  + Bagging)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5107d9b9","outputId":"d39bd21c-8ef9-4da2-8137-b8a7be06f9b6"},"outputs":[{"data":{"text/plain":["RandomForestClassifier()"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["forest = RandomForestClassifier()\n","forest.fit( x_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZ2OSro0nyiO","outputId":"ad4f0900-c23b-485a-de38-2e020291c520"},"outputs":[{"data":{"text/plain":["array([1, 1, 2, 0, 2, 0, 2, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 1, 0,\n","       2, 0, 1, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2, 0])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["forest.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adIBwC96nyiO","outputId":"de30d9bf-007b-4529-c2cd-de3001964f39"},"outputs":[{"data":{"text/plain":["1.0"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["forest.score(x_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"sYevsIAGnyiO"},"source":["# 4.Boosting\n","\n",">여러 개의 알고리즘이 순차적으로 학습을 하되, 앞에 학습한 알고리즘 예측이 틀린 데이터에 대해 올바르게 예측할 수 있도록,    \n","그 다음번 알고리즘에 가중치(Ada)를 부여하여 학습과 예측을 진행하는 방식   \n","잔여오차를 다시학습(gradient)   \n","\n","\n","**부스팅 알고리즘은 대표적으로 아래와 같은 알고리즘들이 있음**\n","- AdaBoost\n","- Gradient Booting Machine(GBM)\n","- XGBoost\n","- LightGBM\n","- CatBoost\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9gRSn8NnyiO","outputId":"53a7b688-83a7-475a-92ad-395295a30419"},"outputs":[{"data":{"text/plain":["Pipeline(steps=[('standardscaler', StandardScaler()),\n","                ('adaboostclassifier', AdaBoostClassifier())])"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["model_ada = make_pipeline(StandardScaler(),AdaBoostClassifier()) \n","# best_estimator에 원하는 모델 넣어도 됨  default = DecisionTree\n","# 이 경우에 DecisionTree에다가 50회의 boosting을 준거임\n","\n","model_ada.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yD9vNPOgnyiO","outputId":"6897a987-d098-407d-aef5-226ec4383929"},"outputs":[{"data":{"text/plain":["array([1, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0,\n","       2, 0, 1, 0, 0, 1, 1, 0, 2, 1, 2, 2, 1, 0])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["model_ada.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umuJajBUnyiP","outputId":"06564493-ed30-49b1-aff3-01a9f6594b68"},"outputs":[{"data":{"text/plain":["0.9166666666666666"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["model_ada.score(x_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0lSINVTynyiP","outputId":"f27192a8-d036-40fe-882f-c68fd7dd76ba"},"outputs":[{"data":{"text/plain":["VotingClassifier(estimators=[('ada', AdaBoostClassifier()),\n","                             ('bc', BaggingClassifier()),\n","                             ('logi', LogisticRegression()),\n","                             ('knn', KNeighborsClassifier())],\n","                 voting='soft')"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["models = [('ada', AdaBoostClassifier()), \n","          ('bc', BaggingClassifier()), \n","          ('logi',LogisticRegression()), \n","          ('knn', KNeighborsClassifier())]\n","          \n","model_vote = VotingClassifier(models, voting='soft')\n","model_vote.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYLGRlP1nyiP","outputId":"a9cd4061-900f-410d-e98d-c6aac022e22c"},"outputs":[{"data":{"text/plain":["0.9444444444444444"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["model_vote.score(x_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"D2K_JeahnyiP"},"source":["## 4-1.xgBoost"]},{"cell_type":"markdown","metadata":{"id":"9QRnikw2nyiQ"},"source":["Booster Parameters\n","> eta [default=0.3] => learning_rate\n","- GBM의 학습 속도와 유사.\n","- 각 단계에서 가중치를 줄임으로써 모델을 더 강건하게 만든다.\n","- 일반적으로 0.01-0.2\n","> min_child_weight [default=1] (Should be tuned using CV)\n","- child의 관측에서 요구되는 최소 가중치의 합\n","- over-fitting vs under-fitting을 조정하기 위한 파라미터.\n","- 너무 큰 값이 주어지면 under-fitting.\n","> max_depth [default=6] (Should be tuned using CV)\n","- 트리의 최대 깊이.\n","- 일반적으로 3-10\n","> max_leaf_nodes\n","- 최종 노드의 최대 개수. (max number of terminal nodes)\n","- 이진 트리가 생성되기 때문에 max_depth가 6이면 max_leaf_nodes는 2^6개가 됨.\n","> gamma [default=0]\n","- 분할을 수행하는데 필요한 최소 손실 감소를 지정한다.\n","- 알고리즘을 보수적으로 만든다. loss function에 따라 조정해야 한다.\n","> subsample [default=1]\n","- 각 트리마다의 관측 데이터 샘플링 비율.\n","- 값을 적게 주면 over-fitting을 방지하지만 값을 너무 작게 주면 under-fitting.\n","- 일반적으로 0.5-1\n","> colsample_bytree [default=1]\n","- 각 트리마다의 feature 샘플링 비율.\n","- 일반적으로 0.5-1\n","> lambda [default=1] => reg_lambda\n","- 가중치에 대한 L2 정규화 용어 (Ridge 회귀 분석과 유사(?))\n","> alpha [default=0] => reg_alpha\n","- 가중치에 대한 L1 정규화 용어 (Lasso 회귀 분석과 유사(?))\n","> scale_pos_weight [default=1]\n","- 불균형한 경우 더 빠른 수렴(convergence)에 도움되므로 0보다 큰 값을 쓸것."]},{"cell_type":"markdown","metadata":{"id":"WnElKfBrnyiQ"},"source":["- binary:logistic : 이진 분류를 위한 로지스틱 회귀, 예측된 확률을 반환한다. (not class)\n","- multi:softmax : softmax를 사용한 다중 클래스 분류, 예측된 클래스를 반환한다. (not probabilities)\n","- multi:softprob : softmax와 같지만 각 클래스에 대한 예상 확률을 반환한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8DCvURa3nyiP"},"outputs":[],"source":["from xgboost import XGBClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jB_dcL8xnyiP","outputId":"dc87a3ba-4871-41b1-f9ba-d0a377c5f61e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[09:49:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"data":{"text/plain":["XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n","              importance_type='gain', interaction_constraints='',\n","              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n","              min_child_weight=1, missing=nan, monotone_constraints='()',\n","              n_estimators=100, n_jobs=20, num_parallel_tree=1, random_state=0,\n","              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n","              tree_method='exact', validate_parameters=1, verbosity=None)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["xgb = XGBClassifier() # default > tree, boost, 2진분류\n","# adaboost에서 성능 업그레이드 한 거\n","xgb.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnHzbLnjnyiR"},"outputs":[],"source":["p = xgb.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePrppPe8nyiR","outputId":"c5bd159e-c6ae-44d5-db46-7a835dadfb38"},"outputs":[{"data":{"text/plain":["0.9722222222222222"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["xgb.score(x_test, y_test)"]}]}